{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BummoK/BummoK/blob/main/Autoencoder_%EB%B3%91%EB%AA%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Aeh4W1Crglc"
      },
      "source": [
        "# 기존 데이터셋 + 차량 7글자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsCufcscrglj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yYKGfQSrgll"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data_before.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBQHp5kKrgll",
        "outputId": "40b9e47b-d4de-4661-b30a-8908fc4aedc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "X1        0\n",
              "X2        0\n",
              "X3        0\n",
              "X6        0\n",
              "X7        0\n",
              "X8        0\n",
              "X9        0\n",
              "X10       0\n",
              "X11       0\n",
              "X12       0\n",
              "X13       0\n",
              "X14       0\n",
              "X15       0\n",
              "X16       0\n",
              "X17       0\n",
              "target    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 결측값 확인\n",
        "df.isnull().sum() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5fdHD6zrglm"
      },
      "outputs": [],
      "source": [
        "df_second = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7-wyUH9rgln"
      },
      "outputs": [],
      "source": [
        "new_X1 = []\n",
        "for i in range(len(df_second)):\n",
        "    new_X1.append(df_second['X1'][i][:7])\n",
        "\n",
        "df_second.insert(1, 'new_X1', new_X1)\n",
        "df_second = df_second.drop(['X1'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub-L-o9yrgln",
        "outputId": "ed835357-6d13-474f-f49e-669a0ef55d08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8816FLU    155757\n",
              "8816FKU    111763\n",
              "8816FMU    104682\n",
              "7816FKU      4935\n",
              "7816FLU      4336\n",
              "7816FMU      2304\n",
              "8816FJU      2298\n",
              "Name: new_X1, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_second['new_X1'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4cYeT0Xrglo"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCg_ceWrrglo"
      },
      "outputs": [],
      "source": [
        "car_encoder = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Mooi6k6rglo"
      },
      "outputs": [],
      "source": [
        "df_second['new_X1'] = car_encoder.fit_transform(df_second['new_X1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psT2x8bZrglo"
      },
      "outputs": [],
      "source": [
        "df_second = df_second.drop(['X2'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TJu8oV_rglp",
        "outputId": "a70c5f8b-388b-4d9a-a632-d9ae29868e48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5    155757\n",
              "4    111763\n",
              "6    104682\n",
              "0      4935\n",
              "1      4336\n",
              "2      2304\n",
              "3      2298\n",
              "Name: new_X1, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_second['new_X1'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCVpFYo8rglp"
      },
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chwO7skarglp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9nlIXRArglq"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSfbik-srglq"
      },
      "outputs": [],
      "source": [
        "scaler = RobustScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ7068Otrglq"
      },
      "outputs": [],
      "source": [
        "second_ae = df_second.copy()\n",
        "second_ae.iloc[:,:-1] = scaler.fit_transform(second_ae.iloc[:,:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvKRfJ23rglq",
        "outputId": "5a171e78-71b6-46fa-bdee-8256bce6e7dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>new_X1</th>\n",
              "      <th>X3</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.5</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.141207</td>\n",
              "      <td>-0.038095</td>\n",
              "      <td>0.794334</td>\n",
              "      <td>-0.031780</td>\n",
              "      <td>0.338237</td>\n",
              "      <td>0.665322</td>\n",
              "      <td>0.187080</td>\n",
              "      <td>-0.065966</td>\n",
              "      <td>-0.171270</td>\n",
              "      <td>1.104331</td>\n",
              "      <td>0.715948</td>\n",
              "      <td>-0.272466</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.5</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.071046</td>\n",
              "      <td>-0.019048</td>\n",
              "      <td>0.632549</td>\n",
              "      <td>0.567409</td>\n",
              "      <td>0.008911</td>\n",
              "      <td>1.095564</td>\n",
              "      <td>0.188204</td>\n",
              "      <td>0.598104</td>\n",
              "      <td>0.346213</td>\n",
              "      <td>1.929018</td>\n",
              "      <td>1.414098</td>\n",
              "      <td>-0.270995</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-2.5</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.080756</td>\n",
              "      <td>0.346032</td>\n",
              "      <td>0.284480</td>\n",
              "      <td>0.421981</td>\n",
              "      <td>0.005518</td>\n",
              "      <td>0.441728</td>\n",
              "      <td>0.189748</td>\n",
              "      <td>0.433572</td>\n",
              "      <td>0.206825</td>\n",
              "      <td>1.037080</td>\n",
              "      <td>0.882806</td>\n",
              "      <td>-0.269642</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-2.5</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.013502</td>\n",
              "      <td>-0.460317</td>\n",
              "      <td>0.762532</td>\n",
              "      <td>0.656858</td>\n",
              "      <td>0.008911</td>\n",
              "      <td>0.555442</td>\n",
              "      <td>0.190208</td>\n",
              "      <td>0.659959</td>\n",
              "      <td>0.068040</td>\n",
              "      <td>0.944658</td>\n",
              "      <td>0.820798</td>\n",
              "      <td>-0.268771</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2.5</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.006431</td>\n",
              "      <td>2.298413</td>\n",
              "      <td>-1.055003</td>\n",
              "      <td>0.170763</td>\n",
              "      <td>0.005363</td>\n",
              "      <td>-0.712294</td>\n",
              "      <td>0.191859</td>\n",
              "      <td>0.155746</td>\n",
              "      <td>0.095054</td>\n",
              "      <td>0.087548</td>\n",
              "      <td>-0.451486</td>\n",
              "      <td>-0.268361</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386070</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.085522</td>\n",
              "      <td>0.469841</td>\n",
              "      <td>-1.618404</td>\n",
              "      <td>-1.756426</td>\n",
              "      <td>-0.086899</td>\n",
              "      <td>-0.755514</td>\n",
              "      <td>-0.767408</td>\n",
              "      <td>-1.765025</td>\n",
              "      <td>1.037901</td>\n",
              "      <td>-0.079570</td>\n",
              "      <td>-1.022391</td>\n",
              "      <td>-0.725790</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386071</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.042850</td>\n",
              "      <td>-0.393651</td>\n",
              "      <td>0.308709</td>\n",
              "      <td>-1.040594</td>\n",
              "      <td>0.308947</td>\n",
              "      <td>0.804596</td>\n",
              "      <td>-0.766487</td>\n",
              "      <td>-1.029819</td>\n",
              "      <td>0.473461</td>\n",
              "      <td>-0.081888</td>\n",
              "      <td>-0.418968</td>\n",
              "      <td>-0.725751</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386072</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.068390</td>\n",
              "      <td>-0.457143</td>\n",
              "      <td>0.289582</td>\n",
              "      <td>-0.534631</td>\n",
              "      <td>0.381667</td>\n",
              "      <td>0.087350</td>\n",
              "      <td>-0.766487</td>\n",
              "      <td>-0.515058</td>\n",
              "      <td>0.433811</td>\n",
              "      <td>-0.080976</td>\n",
              "      <td>-0.282481</td>\n",
              "      <td>-0.725676</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386073</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.058294</td>\n",
              "      <td>0.434921</td>\n",
              "      <td>0.090611</td>\n",
              "      <td>0.199426</td>\n",
              "      <td>-0.090794</td>\n",
              "      <td>0.153979</td>\n",
              "      <td>-0.766487</td>\n",
              "      <td>0.209138</td>\n",
              "      <td>0.189606</td>\n",
              "      <td>1.612558</td>\n",
              "      <td>0.247642</td>\n",
              "      <td>-0.725402</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386074</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.138723</td>\n",
              "      <td>-0.495238</td>\n",
              "      <td>-2.998507</td>\n",
              "      <td>-0.018688</td>\n",
              "      <td>-0.821369</td>\n",
              "      <td>-1.313203</td>\n",
              "      <td>-0.766487</td>\n",
              "      <td>0.194840</td>\n",
              "      <td>0.198768</td>\n",
              "      <td>0.002912</td>\n",
              "      <td>-1.022391</td>\n",
              "      <td>-0.725089</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>386075 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        new_X1   X3        X6        X7        X8        X9       X10  \\\n",
              "0         -2.5 -0.5  0.141207 -0.038095  0.794334 -0.031780  0.338237   \n",
              "1         -2.5 -0.5  0.071046 -0.019048  0.632549  0.567409  0.008911   \n",
              "2         -2.5 -0.5  0.080756  0.346032  0.284480  0.421981  0.005518   \n",
              "3         -2.5 -0.5  0.013502 -0.460317  0.762532  0.656858  0.008911   \n",
              "4         -2.5 -0.5  0.006431  2.298413 -1.055003  0.170763  0.005363   \n",
              "...        ...  ...       ...       ...       ...       ...       ...   \n",
              "386070     0.5  0.5  0.085522  0.469841 -1.618404 -1.756426 -0.086899   \n",
              "386071     0.5  0.5  0.042850 -0.393651  0.308709 -1.040594  0.308947   \n",
              "386072     0.5  0.5  0.068390 -0.457143  0.289582 -0.534631  0.381667   \n",
              "386073     0.5  0.5  0.058294  0.434921  0.090611  0.199426 -0.090794   \n",
              "386074     0.5  0.5  0.138723 -0.495238 -2.998507 -0.018688 -0.821369   \n",
              "\n",
              "             X11       X12       X13       X14       X15       X16       X17  \\\n",
              "0       0.665322  0.187080 -0.065966 -0.171270  1.104331  0.715948 -0.272466   \n",
              "1       1.095564  0.188204  0.598104  0.346213  1.929018  1.414098 -0.270995   \n",
              "2       0.441728  0.189748  0.433572  0.206825  1.037080  0.882806 -0.269642   \n",
              "3       0.555442  0.190208  0.659959  0.068040  0.944658  0.820798 -0.268771   \n",
              "4      -0.712294  0.191859  0.155746  0.095054  0.087548 -0.451486 -0.268361   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "386070 -0.755514 -0.767408 -1.765025  1.037901 -0.079570 -1.022391 -0.725790   \n",
              "386071  0.804596 -0.766487 -1.029819  0.473461 -0.081888 -0.418968 -0.725751   \n",
              "386072  0.087350 -0.766487 -0.515058  0.433811 -0.080976 -0.282481 -0.725676   \n",
              "386073  0.153979 -0.766487  0.209138  0.189606  1.612558  0.247642 -0.725402   \n",
              "386074 -1.313203 -0.766487  0.194840  0.198768  0.002912 -1.022391 -0.725089   \n",
              "\n",
              "        target  \n",
              "0            0  \n",
              "1            0  \n",
              "2            0  \n",
              "3            0  \n",
              "4            0  \n",
              "...        ...  \n",
              "386070       0  \n",
              "386071       0  \n",
              "386072       0  \n",
              "386073       0  \n",
              "386074       0  \n",
              "\n",
              "[386075 rows x 15 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "second_ae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUv6mE59rglr"
      },
      "outputs": [],
      "source": [
        "TRAINING_SAMPLE = 473\n",
        "VALIDATE_SIZE = 0.2\n",
        "RANDOM_SEED = 1\n",
        "abnormal = second_ae[second_ae.target == 1]\n",
        "normal = second_ae[second_ae.target == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T8c3CNerglr"
      },
      "outputs": [],
      "source": [
        "# shuffle our training set\n",
        "normal = normal.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "\n",
        "# training set: exlusively non-fraud transactions\n",
        "X_train = normal.iloc[TRAINING_SAMPLE:].drop('target', axis=1)\n",
        "\n",
        "# testing  set: the remaining non-fraud + all the fraud \n",
        "X_test = normal.iloc[:TRAINING_SAMPLE].append(abnormal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDk3PgaWrglr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train // validate - no labels since they're all clean anyway\n",
        "X_train, X_validate = train_test_split(X_train, \n",
        "                                       test_size=VALIDATE_SIZE, \n",
        "                                       random_state=RANDOM_SEED)\n",
        "\n",
        "# manually splitting the labels from the test df\n",
        "X_test, y_test = X_test.drop('target', axis=1).values, X_test.target.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeoMAKO4rgls"
      },
      "outputs": [],
      "source": [
        "import random as rn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1Am-laPrgls"
      },
      "outputs": [],
      "source": [
        "# setting random seeds for libraries to ensure reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "rn.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QUEBCyargls",
        "outputId": "5da30c2e-681e-479e-b51e-c5fb133b683d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 14)                210       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 120       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 8)                32        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8)                 40        \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 14)                126       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 580\n",
            "Trainable params: 556\n",
            "Non-trainable params: 24\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_dim = X_train.shape[1]\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 500\n",
        "\n",
        "# https://keras.io/layers/core/\n",
        "autoencoder = tf.keras.models.Sequential([\n",
        "    \n",
        "    # deconstruct / encode\n",
        "    tf.keras.layers.Dense(input_dim, activation='relu', input_shape=(input_dim, )), \n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "    \n",
        "    # reconstruction / decode\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(input_dim, activation='relu')\n",
        "    \n",
        "])\n",
        "\n",
        "# https://keras.io/api/models/model_training_apis/\n",
        "autoencoder.compile(optimizer=\"adam\", \n",
        "                    loss=\"mse\",\n",
        "                    metrics=[\"acc\"])\n",
        "\n",
        "# print an overview of our model\n",
        "autoencoder.summary();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FW1zpRwrglt"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "yyyymmddHHMM = datetime.now().strftime('%Y%m%d%H%M')\n",
        "\n",
        "# new folder for a new run\n",
        "log_subdir = f'{yyyymmddHHMM}_batch{BATCH_SIZE}_layers{len(autoencoder.layers)}'\n",
        "\n",
        "# define our early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.0001,\n",
        "    patience=50,\n",
        "    verbose=1, \n",
        "    mode='min',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "save_model = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='autoencoder_best_weights.hdf5',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    verbose=0,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# callbacks argument only takes a list\n",
        "cb = [early_stop, save_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZvnVC38rglw",
        "outputId": "4e1cd3bf-583e-4d7e-c147-820c16973519"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "602/602 [==============================] - 2s 2ms/step - loss: 0.6211 - acc: 0.3772 - val_loss: 0.5092 - val_acc: 0.4550\n",
            "Epoch 2/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4759 - acc: 0.5060 - val_loss: 0.4566 - val_acc: 0.6393\n",
            "Epoch 3/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4455 - acc: 0.6508 - val_loss: 0.4389 - val_acc: 0.6659\n",
            "Epoch 4/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4368 - acc: 0.6671 - val_loss: 0.4335 - val_acc: 0.6829\n",
            "Epoch 5/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4320 - acc: 0.6865 - val_loss: 0.4287 - val_acc: 0.6969\n",
            "Epoch 6/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4282 - acc: 0.6915 - val_loss: 0.4263 - val_acc: 0.7019\n",
            "Epoch 7/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4263 - acc: 0.6989 - val_loss: 0.4254 - val_acc: 0.7131\n",
            "Epoch 8/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4253 - acc: 0.7065 - val_loss: 0.4239 - val_acc: 0.7144\n",
            "Epoch 9/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4244 - acc: 0.7132 - val_loss: 0.4247 - val_acc: 0.7090\n",
            "Epoch 10/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4234 - acc: 0.7192 - val_loss: 0.4226 - val_acc: 0.7266\n",
            "Epoch 11/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4229 - acc: 0.7247 - val_loss: 0.4221 - val_acc: 0.7332\n",
            "Epoch 12/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4224 - acc: 0.7267 - val_loss: 0.4219 - val_acc: 0.7346\n",
            "Epoch 13/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4221 - acc: 0.7300 - val_loss: 0.4216 - val_acc: 0.7353\n",
            "Epoch 14/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4219 - acc: 0.7327 - val_loss: 0.4219 - val_acc: 0.7363\n",
            "Epoch 15/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4214 - acc: 0.7368 - val_loss: 0.4209 - val_acc: 0.7426\n",
            "Epoch 16/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4212 - acc: 0.7404 - val_loss: 0.4207 - val_acc: 0.7463\n",
            "Epoch 17/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4209 - acc: 0.7465 - val_loss: 0.4213 - val_acc: 0.7480\n",
            "Epoch 18/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4206 - acc: 0.7569 - val_loss: 0.4203 - val_acc: 0.7734\n",
            "Epoch 19/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4206 - acc: 0.7657 - val_loss: 0.4253 - val_acc: 0.7543\n",
            "Epoch 20/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4204 - acc: 0.7709 - val_loss: 0.4201 - val_acc: 0.7632\n",
            "Epoch 21/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4202 - acc: 0.7742 - val_loss: 0.4197 - val_acc: 0.7822\n",
            "Epoch 22/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4202 - acc: 0.7767 - val_loss: 0.4197 - val_acc: 0.7892\n",
            "Epoch 23/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4200 - acc: 0.7768 - val_loss: 0.4194 - val_acc: 0.7905\n",
            "Epoch 24/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4200 - acc: 0.7776 - val_loss: 0.4195 - val_acc: 0.7904\n",
            "Epoch 25/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4198 - acc: 0.7781 - val_loss: 0.4215 - val_acc: 0.7745\n",
            "Epoch 26/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4198 - acc: 0.7804 - val_loss: 0.4194 - val_acc: 0.7928\n",
            "Epoch 27/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4198 - acc: 0.7807 - val_loss: 0.4196 - val_acc: 0.7632\n",
            "Epoch 28/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4197 - acc: 0.7812 - val_loss: 0.4197 - val_acc: 0.7892\n",
            "Epoch 29/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4196 - acc: 0.7829 - val_loss: 0.4236 - val_acc: 0.7529\n",
            "Epoch 30/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4196 - acc: 0.7815 - val_loss: 0.4213 - val_acc: 0.7766\n",
            "Epoch 31/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4194 - acc: 0.7803 - val_loss: 0.4191 - val_acc: 0.7931\n",
            "Epoch 32/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4195 - acc: 0.7817 - val_loss: 0.4190 - val_acc: 0.7890\n",
            "Epoch 33/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4196 - acc: 0.7807 - val_loss: 0.4193 - val_acc: 0.7933\n",
            "Epoch 34/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4192 - acc: 0.7819 - val_loss: 0.4199 - val_acc: 0.7772\n",
            "Epoch 35/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4193 - acc: 0.7807 - val_loss: 0.4228 - val_acc: 0.7520\n",
            "Epoch 36/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4192 - acc: 0.7795 - val_loss: 0.4191 - val_acc: 0.7493\n",
            "Epoch 37/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4189 - acc: 0.7834 - val_loss: 0.4189 - val_acc: 0.7965\n",
            "Epoch 38/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4189 - acc: 0.7808 - val_loss: 0.4184 - val_acc: 0.7601\n",
            "Epoch 39/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4189 - acc: 0.7823 - val_loss: 0.4186 - val_acc: 0.7786\n",
            "Epoch 40/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4189 - acc: 0.7797 - val_loss: 0.4188 - val_acc: 0.7887\n",
            "Epoch 41/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4189 - acc: 0.7805 - val_loss: 0.4183 - val_acc: 0.7572\n",
            "Epoch 42/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4187 - acc: 0.7813 - val_loss: 0.4182 - val_acc: 0.8047\n",
            "Epoch 43/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4186 - acc: 0.7831 - val_loss: 0.4188 - val_acc: 0.8028\n",
            "Epoch 44/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4185 - acc: 0.7803 - val_loss: 0.4182 - val_acc: 0.7783\n",
            "Epoch 45/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4185 - acc: 0.7778 - val_loss: 0.4182 - val_acc: 0.8063\n",
            "Epoch 46/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4184 - acc: 0.7791 - val_loss: 0.4180 - val_acc: 0.7844\n",
            "Epoch 47/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4185 - acc: 0.7769 - val_loss: 0.4181 - val_acc: 0.7583\n",
            "Epoch 48/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4184 - acc: 0.7785 - val_loss: 0.4180 - val_acc: 0.7967\n",
            "Epoch 49/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4182 - acc: 0.7797 - val_loss: 0.4179 - val_acc: 0.8070\n",
            "Epoch 50/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4183 - acc: 0.7828 - val_loss: 0.4182 - val_acc: 0.8086\n",
            "Epoch 51/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4184 - acc: 0.7793 - val_loss: 0.4179 - val_acc: 0.7576\n",
            "Epoch 52/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4182 - acc: 0.7812 - val_loss: 0.4177 - val_acc: 0.8066\n",
            "Epoch 53/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4181 - acc: 0.7843 - val_loss: 0.4177 - val_acc: 0.8142\n",
            "Epoch 54/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4181 - acc: 0.7801 - val_loss: 0.4182 - val_acc: 0.8102\n",
            "Epoch 55/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4181 - acc: 0.7773 - val_loss: 0.4179 - val_acc: 0.7560\n",
            "Epoch 56/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4181 - acc: 0.7811 - val_loss: 0.4177 - val_acc: 0.7635\n",
            "Epoch 57/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4180 - acc: 0.7791 - val_loss: 0.4176 - val_acc: 0.7770\n",
            "Epoch 58/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4181 - acc: 0.7795 - val_loss: 0.4175 - val_acc: 0.7811\n",
            "Epoch 59/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4180 - acc: 0.7843 - val_loss: 0.4178 - val_acc: 0.7607\n",
            "Epoch 60/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4178 - acc: 0.7828 - val_loss: 0.4179 - val_acc: 0.7619\n",
            "Epoch 61/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4178 - acc: 0.7837 - val_loss: 0.4174 - val_acc: 0.7676\n",
            "Epoch 62/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4178 - acc: 0.7859 - val_loss: 0.4176 - val_acc: 0.7886\n",
            "Epoch 63/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4178 - acc: 0.7870 - val_loss: 0.4180 - val_acc: 0.8044\n",
            "Epoch 64/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4176 - acc: 0.7901 - val_loss: 0.4174 - val_acc: 0.8153\n",
            "Epoch 65/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4178 - acc: 0.7864 - val_loss: 0.4173 - val_acc: 0.7760\n",
            "Epoch 66/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4178 - acc: 0.7897 - val_loss: 0.4173 - val_acc: 0.8090\n",
            "Epoch 67/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4177 - acc: 0.7902 - val_loss: 0.4185 - val_acc: 0.7743\n",
            "Epoch 68/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4178 - acc: 0.7908 - val_loss: 0.4174 - val_acc: 0.8181\n",
            "Epoch 69/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4176 - acc: 0.7923 - val_loss: 0.4177 - val_acc: 0.8057\n",
            "Epoch 70/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4176 - acc: 0.7939 - val_loss: 0.4174 - val_acc: 0.7985\n",
            "Epoch 71/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4176 - acc: 0.7930 - val_loss: 0.4176 - val_acc: 0.7987\n",
            "Epoch 72/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4176 - acc: 0.7944 - val_loss: 0.4173 - val_acc: 0.8120\n",
            "Epoch 73/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4176 - acc: 0.7953 - val_loss: 0.4170 - val_acc: 0.8164\n",
            "Epoch 74/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4175 - acc: 0.7959 - val_loss: 0.4175 - val_acc: 0.8123\n",
            "Epoch 75/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4176 - acc: 0.7970 - val_loss: 0.4173 - val_acc: 0.8127\n",
            "Epoch 76/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4175 - acc: 0.7969 - val_loss: 0.4176 - val_acc: 0.8108\n",
            "Epoch 77/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4175 - acc: 0.7916 - val_loss: 0.4170 - val_acc: 0.8132\n",
            "Epoch 78/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4175 - acc: 0.7883 - val_loss: 0.4170 - val_acc: 0.7829\n",
            "Epoch 79/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4175 - acc: 0.7890 - val_loss: 0.4170 - val_acc: 0.8068\n",
            "Epoch 80/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4174 - acc: 0.7887 - val_loss: 0.4173 - val_acc: 0.8100\n",
            "Epoch 81/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4175 - acc: 0.7884 - val_loss: 0.4170 - val_acc: 0.8137\n",
            "Epoch 82/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4174 - acc: 0.7862 - val_loss: 0.4170 - val_acc: 0.8104\n",
            "Epoch 83/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4175 - acc: 0.7871 - val_loss: 0.4172 - val_acc: 0.8042\n",
            "Epoch 84/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4173 - acc: 0.7867 - val_loss: 0.4169 - val_acc: 0.7991\n",
            "Epoch 85/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4172 - acc: 0.7862 - val_loss: 0.4168 - val_acc: 0.8039\n",
            "Epoch 86/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4172 - acc: 0.7874 - val_loss: 0.4176 - val_acc: 0.8073\n",
            "Epoch 87/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4173 - acc: 0.7893 - val_loss: 0.4170 - val_acc: 0.8083\n",
            "Epoch 88/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4172 - acc: 0.7883 - val_loss: 0.4176 - val_acc: 0.7952\n",
            "Epoch 89/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4173 - acc: 0.7892 - val_loss: 0.4167 - val_acc: 0.8138\n",
            "Epoch 90/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4172 - acc: 0.7893 - val_loss: 0.4172 - val_acc: 0.7914\n",
            "Epoch 91/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4173 - acc: 0.7859 - val_loss: 0.4173 - val_acc: 0.7889\n",
            "Epoch 92/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7872 - val_loss: 0.4171 - val_acc: 0.7985\n",
            "Epoch 93/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4172 - acc: 0.7877 - val_loss: 0.4171 - val_acc: 0.7840\n",
            "Epoch 94/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7891 - val_loss: 0.4168 - val_acc: 0.8038\n",
            "Epoch 95/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4172 - acc: 0.7884 - val_loss: 0.4185 - val_acc: 0.7872\n",
            "Epoch 96/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4172 - acc: 0.7879 - val_loss: 0.4169 - val_acc: 0.7825\n",
            "Epoch 97/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7883 - val_loss: 0.4172 - val_acc: 0.8031\n",
            "Epoch 98/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7884 - val_loss: 0.4203 - val_acc: 0.7742\n",
            "Epoch 99/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4172 - acc: 0.7883 - val_loss: 0.4169 - val_acc: 0.8048\n",
            "Epoch 100/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7888 - val_loss: 0.4167 - val_acc: 0.7989\n",
            "Epoch 101/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7883 - val_loss: 0.4182 - val_acc: 0.7842\n",
            "Epoch 102/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7888 - val_loss: 0.4171 - val_acc: 0.7805\n",
            "Epoch 103/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7889 - val_loss: 0.4167 - val_acc: 0.8125\n",
            "Epoch 104/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7893 - val_loss: 0.4168 - val_acc: 0.7938\n",
            "Epoch 105/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7884 - val_loss: 0.4169 - val_acc: 0.7889\n",
            "Epoch 106/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7864 - val_loss: 0.4173 - val_acc: 0.7890\n",
            "Epoch 107/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7878 - val_loss: 0.4175 - val_acc: 0.7968\n",
            "Epoch 108/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7904 - val_loss: 0.4169 - val_acc: 0.7924\n",
            "Epoch 109/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7882 - val_loss: 0.4173 - val_acc: 0.7866\n",
            "Epoch 110/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7878 - val_loss: 0.4169 - val_acc: 0.8011\n",
            "Epoch 111/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7900 - val_loss: 0.4167 - val_acc: 0.8025\n",
            "Epoch 112/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7889 - val_loss: 0.4190 - val_acc: 0.7680\n",
            "Epoch 113/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7900 - val_loss: 0.4168 - val_acc: 0.8022\n",
            "Epoch 114/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7885 - val_loss: 0.4167 - val_acc: 0.8004\n",
            "Epoch 115/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7898 - val_loss: 0.4167 - val_acc: 0.7997\n",
            "Epoch 116/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7904 - val_loss: 0.4170 - val_acc: 0.8017\n",
            "Epoch 117/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7908 - val_loss: 0.4166 - val_acc: 0.8150\n",
            "Epoch 118/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7910 - val_loss: 0.4168 - val_acc: 0.8083\n",
            "Epoch 119/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7920 - val_loss: 0.4241 - val_acc: 0.7508\n",
            "Epoch 120/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7891 - val_loss: 0.4180 - val_acc: 0.7899\n",
            "Epoch 121/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7886 - val_loss: 0.4193 - val_acc: 0.7607\n",
            "Epoch 122/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7909 - val_loss: 0.4169 - val_acc: 0.7967\n",
            "Epoch 123/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7911 - val_loss: 0.4168 - val_acc: 0.8030\n",
            "Epoch 124/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7908 - val_loss: 0.4167 - val_acc: 0.8094\n",
            "Epoch 125/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7909 - val_loss: 0.4170 - val_acc: 0.7894\n",
            "Epoch 126/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7881 - val_loss: 0.4167 - val_acc: 0.7908\n",
            "Epoch 127/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7855 - val_loss: 0.4168 - val_acc: 0.7980\n",
            "Epoch 128/500\n",
            "602/602 [==============================] - 2s 3ms/step - loss: 0.4169 - acc: 0.7848 - val_loss: 0.4168 - val_acc: 0.7679\n",
            "Epoch 129/500\n",
            "602/602 [==============================] - 2s 3ms/step - loss: 0.4170 - acc: 0.7858 - val_loss: 0.4170 - val_acc: 0.7699\n",
            "Epoch 130/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7871 - val_loss: 0.4166 - val_acc: 0.8158\n",
            "Epoch 131/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7897 - val_loss: 0.4166 - val_acc: 0.8102\n",
            "Epoch 132/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4171 - acc: 0.7911 - val_loss: 0.4170 - val_acc: 0.7924\n",
            "Epoch 133/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7914 - val_loss: 0.4169 - val_acc: 0.8072\n",
            "Epoch 134/500\n",
            "602/602 [==============================] - 2s 3ms/step - loss: 0.4171 - acc: 0.7912 - val_loss: 0.4167 - val_acc: 0.8168\n",
            "Epoch 135/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7903 - val_loss: 0.4168 - val_acc: 0.8106\n",
            "Epoch 136/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7883 - val_loss: 0.4166 - val_acc: 0.7974\n",
            "Epoch 137/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7900 - val_loss: 0.4167 - val_acc: 0.8164\n",
            "Epoch 138/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7874 - val_loss: 0.4167 - val_acc: 0.8158\n",
            "Epoch 139/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7902 - val_loss: 0.4169 - val_acc: 0.8093\n",
            "Epoch 140/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7884 - val_loss: 0.4166 - val_acc: 0.7937\n",
            "Epoch 141/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7887 - val_loss: 0.4169 - val_acc: 0.8047\n",
            "Epoch 142/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7899 - val_loss: 0.4171 - val_acc: 0.8025\n",
            "Epoch 143/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7891 - val_loss: 0.4169 - val_acc: 0.7995\n",
            "Epoch 144/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7880 - val_loss: 0.4169 - val_acc: 0.7577\n",
            "Epoch 145/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7877 - val_loss: 0.4167 - val_acc: 0.8055\n",
            "Epoch 146/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7900 - val_loss: 0.4169 - val_acc: 0.8018\n",
            "Epoch 147/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7894 - val_loss: 0.4169 - val_acc: 0.7952\n",
            "Epoch 148/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7907 - val_loss: 0.4165 - val_acc: 0.8103\n",
            "Epoch 149/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7912 - val_loss: 0.4170 - val_acc: 0.7769\n",
            "Epoch 150/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7907 - val_loss: 0.4166 - val_acc: 0.8117\n",
            "Epoch 151/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7927 - val_loss: 0.4167 - val_acc: 0.8024\n",
            "Epoch 152/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7923 - val_loss: 0.4168 - val_acc: 0.8110\n",
            "Epoch 153/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7902 - val_loss: 0.4169 - val_acc: 0.7973\n",
            "Epoch 154/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7901 - val_loss: 0.4170 - val_acc: 0.8016\n",
            "Epoch 155/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7873 - val_loss: 0.4167 - val_acc: 0.8135\n",
            "Epoch 156/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4170 - acc: 0.7834 - val_loss: 0.4165 - val_acc: 0.7741\n",
            "Epoch 157/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7850 - val_loss: 0.4175 - val_acc: 0.7761\n",
            "Epoch 158/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7863 - val_loss: 0.4166 - val_acc: 0.8115\n",
            "Epoch 159/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7894 - val_loss: 0.4165 - val_acc: 0.8107\n",
            "Epoch 160/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7911 - val_loss: 0.4166 - val_acc: 0.7994\n",
            "Epoch 161/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7892 - val_loss: 0.4166 - val_acc: 0.7956\n",
            "Epoch 162/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7866 - val_loss: 0.4165 - val_acc: 0.7959\n",
            "Epoch 163/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7889 - val_loss: 0.4165 - val_acc: 0.8102\n",
            "Epoch 164/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7894 - val_loss: 0.4165 - val_acc: 0.8087\n",
            "Epoch 165/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7883 - val_loss: 0.4166 - val_acc: 0.7928\n",
            "Epoch 166/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4167 - acc: 0.7895 - val_loss: 0.4189 - val_acc: 0.7945\n",
            "Epoch 167/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7915 - val_loss: 0.4168 - val_acc: 0.7992\n",
            "Epoch 168/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4169 - acc: 0.7896 - val_loss: 0.4165 - val_acc: 0.8057\n",
            "Epoch 169/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7897 - val_loss: 0.4168 - val_acc: 0.7774\n",
            "Epoch 170/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4166 - acc: 0.7911 - val_loss: 0.4165 - val_acc: 0.8037\n",
            "Epoch 171/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7886 - val_loss: 0.4166 - val_acc: 0.8076\n",
            "Epoch 172/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7874 - val_loss: 0.4168 - val_acc: 0.7793\n",
            "Epoch 173/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7875 - val_loss: 0.4164 - val_acc: 0.8120\n",
            "Epoch 174/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4167 - acc: 0.7887 - val_loss: 0.4165 - val_acc: 0.8150\n",
            "Epoch 175/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7899 - val_loss: 0.4164 - val_acc: 0.8076\n",
            "Epoch 176/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4166 - acc: 0.7824 - val_loss: 0.4165 - val_acc: 0.8152\n",
            "Epoch 177/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4167 - acc: 0.7840 - val_loss: 0.4165 - val_acc: 0.8216\n",
            "Epoch 178/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4167 - acc: 0.7867 - val_loss: 0.4164 - val_acc: 0.8061\n",
            "Epoch 179/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4167 - acc: 0.7864 - val_loss: 0.4192 - val_acc: 0.7847\n",
            "Epoch 180/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7852 - val_loss: 0.4183 - val_acc: 0.7866\n",
            "Epoch 181/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4167 - acc: 0.7866 - val_loss: 0.4164 - val_acc: 0.7824\n",
            "Epoch 182/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4167 - acc: 0.7861 - val_loss: 0.4165 - val_acc: 0.7997\n",
            "Epoch 183/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4168 - acc: 0.7855 - val_loss: 0.4164 - val_acc: 0.8052\n",
            "Epoch 184/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4166 - acc: 0.7880 - val_loss: 0.4167 - val_acc: 0.7999\n",
            "Epoch 185/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4166 - acc: 0.7867 - val_loss: 0.4164 - val_acc: 0.8038\n",
            "Epoch 186/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4167 - acc: 0.7833 - val_loss: 0.4165 - val_acc: 0.7806\n",
            "Epoch 187/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4166 - acc: 0.7834 - val_loss: 0.4166 - val_acc: 0.8042\n",
            "Epoch 188/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4166 - acc: 0.7822 - val_loss: 0.4173 - val_acc: 0.7867\n",
            "Epoch 189/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4166 - acc: 0.7811 - val_loss: 0.4167 - val_acc: 0.7895\n",
            "Epoch 190/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4163 - acc: 0.7825 - val_loss: 0.4161 - val_acc: 0.8059\n",
            "Epoch 191/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4163 - acc: 0.7855 - val_loss: 0.4159 - val_acc: 0.7931\n",
            "Epoch 192/500\n",
            "602/602 [==============================] - 2s 3ms/step - loss: 0.4161 - acc: 0.7833 - val_loss: 0.4162 - val_acc: 0.7906\n",
            "Epoch 193/500\n",
            "602/602 [==============================] - 2s 3ms/step - loss: 0.4161 - acc: 0.7836 - val_loss: 0.4158 - val_acc: 0.7930\n",
            "Epoch 194/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4159 - acc: 0.7844 - val_loss: 0.4157 - val_acc: 0.8139\n",
            "Epoch 195/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4159 - acc: 0.7855 - val_loss: 0.4158 - val_acc: 0.7958\n",
            "Epoch 196/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4159 - acc: 0.7855 - val_loss: 0.4157 - val_acc: 0.7804\n",
            "Epoch 197/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4159 - acc: 0.7840 - val_loss: 0.4161 - val_acc: 0.7926\n",
            "Epoch 198/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4158 - acc: 0.7863 - val_loss: 0.4157 - val_acc: 0.7654\n",
            "Epoch 199/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7868 - val_loss: 0.4156 - val_acc: 0.7857\n",
            "Epoch 200/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7866 - val_loss: 0.4156 - val_acc: 0.7783\n",
            "Epoch 201/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4159 - acc: 0.7855 - val_loss: 0.4164 - val_acc: 0.7698\n",
            "Epoch 202/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4158 - acc: 0.7849 - val_loss: 0.4159 - val_acc: 0.7912\n",
            "Epoch 203/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7870 - val_loss: 0.4163 - val_acc: 0.7765\n",
            "Epoch 204/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4158 - acc: 0.7867 - val_loss: 0.4175 - val_acc: 0.7843\n",
            "Epoch 205/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7851 - val_loss: 0.4156 - val_acc: 0.7944\n",
            "Epoch 206/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4158 - acc: 0.7861 - val_loss: 0.4156 - val_acc: 0.7966\n",
            "Epoch 207/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7842 - val_loss: 0.4157 - val_acc: 0.8021\n",
            "Epoch 208/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4158 - acc: 0.7859 - val_loss: 0.4156 - val_acc: 0.8183\n",
            "Epoch 209/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7864 - val_loss: 0.4157 - val_acc: 0.7926\n",
            "Epoch 210/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4158 - acc: 0.7854 - val_loss: 0.4171 - val_acc: 0.7551\n",
            "Epoch 211/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4158 - acc: 0.7864 - val_loss: 0.4159 - val_acc: 0.8116\n",
            "Epoch 212/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7879 - val_loss: 0.4155 - val_acc: 0.8174\n",
            "Epoch 213/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7881 - val_loss: 0.4156 - val_acc: 0.8095\n",
            "Epoch 214/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7885 - val_loss: 0.4157 - val_acc: 0.7965\n",
            "Epoch 215/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4158 - acc: 0.7853 - val_loss: 0.4158 - val_acc: 0.8161\n",
            "Epoch 216/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4156 - acc: 0.7832 - val_loss: 0.4173 - val_acc: 0.7973\n",
            "Epoch 217/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7840 - val_loss: 0.4157 - val_acc: 0.8161\n",
            "Epoch 218/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4156 - acc: 0.7839 - val_loss: 0.4158 - val_acc: 0.7643\n",
            "Epoch 219/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4156 - acc: 0.7838 - val_loss: 0.4158 - val_acc: 0.7816\n",
            "Epoch 220/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7853 - val_loss: 0.4155 - val_acc: 0.8078\n",
            "Epoch 221/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4156 - acc: 0.7859 - val_loss: 0.4156 - val_acc: 0.8007\n",
            "Epoch 222/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7854 - val_loss: 0.4156 - val_acc: 0.7993\n",
            "Epoch 223/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4156 - acc: 0.7867 - val_loss: 0.4160 - val_acc: 0.8017\n",
            "Epoch 224/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4157 - acc: 0.7859 - val_loss: 0.4161 - val_acc: 0.7876\n",
            "Epoch 225/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4156 - acc: 0.7858 - val_loss: 0.4156 - val_acc: 0.7987\n",
            "Epoch 226/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4155 - acc: 0.7880 - val_loss: 0.4155 - val_acc: 0.7987\n",
            "Epoch 227/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4155 - acc: 0.7874 - val_loss: 0.4160 - val_acc: 0.7991\n",
            "Epoch 228/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4155 - acc: 0.7875 - val_loss: 0.4153 - val_acc: 0.8004\n",
            "Epoch 229/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4155 - acc: 0.7872 - val_loss: 0.4155 - val_acc: 0.8085\n",
            "Epoch 230/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4155 - acc: 0.7868 - val_loss: 0.4159 - val_acc: 0.8012\n",
            "Epoch 231/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7860 - val_loss: 0.4154 - val_acc: 0.7933\n",
            "Epoch 232/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4155 - acc: 0.7884 - val_loss: 0.4154 - val_acc: 0.8030\n",
            "Epoch 233/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7906 - val_loss: 0.4153 - val_acc: 0.8011\n",
            "Epoch 234/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4155 - acc: 0.7870 - val_loss: 0.4226 - val_acc: 0.7245\n",
            "Epoch 235/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4156 - acc: 0.7859 - val_loss: 0.4156 - val_acc: 0.7681\n",
            "Epoch 236/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4156 - acc: 0.7880 - val_loss: 0.4154 - val_acc: 0.8039\n",
            "Epoch 237/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4155 - acc: 0.7866 - val_loss: 0.4155 - val_acc: 0.8152\n",
            "Epoch 238/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7867 - val_loss: 0.4152 - val_acc: 0.7987\n",
            "Epoch 239/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7903 - val_loss: 0.4159 - val_acc: 0.7955\n",
            "Epoch 240/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7906 - val_loss: 0.4152 - val_acc: 0.8057\n",
            "Epoch 241/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7906 - val_loss: 0.4156 - val_acc: 0.7965\n",
            "Epoch 242/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4155 - acc: 0.7900 - val_loss: 0.4153 - val_acc: 0.8046\n",
            "Epoch 243/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7849 - val_loss: 0.4153 - val_acc: 0.7957\n",
            "Epoch 244/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7850 - val_loss: 0.4155 - val_acc: 0.8226\n",
            "Epoch 245/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7837 - val_loss: 0.4155 - val_acc: 0.8229\n",
            "Epoch 246/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7848 - val_loss: 0.4151 - val_acc: 0.7731\n",
            "Epoch 247/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7847 - val_loss: 0.4159 - val_acc: 0.7823\n",
            "Epoch 248/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7828 - val_loss: 0.4170 - val_acc: 0.7969\n",
            "Epoch 249/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7813 - val_loss: 0.4155 - val_acc: 0.7665\n",
            "Epoch 250/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7816 - val_loss: 0.4153 - val_acc: 0.7663\n",
            "Epoch 251/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7835 - val_loss: 0.4154 - val_acc: 0.8199\n",
            "Epoch 252/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7845 - val_loss: 0.4156 - val_acc: 0.8170\n",
            "Epoch 253/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7839 - val_loss: 0.4152 - val_acc: 0.8242\n",
            "Epoch 254/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7863 - val_loss: 0.4154 - val_acc: 0.8224\n",
            "Epoch 255/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4154 - acc: 0.7845 - val_loss: 0.4154 - val_acc: 0.8245\n",
            "Epoch 256/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7836 - val_loss: 0.4159 - val_acc: 0.7884\n",
            "Epoch 257/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7821 - val_loss: 0.4154 - val_acc: 0.7778\n",
            "Epoch 258/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7835 - val_loss: 0.4152 - val_acc: 0.8228\n",
            "Epoch 259/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7828 - val_loss: 0.4152 - val_acc: 0.8251\n",
            "Epoch 260/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7886 - val_loss: 0.4151 - val_acc: 0.7741\n",
            "Epoch 261/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7865 - val_loss: 0.4151 - val_acc: 0.7875\n",
            "Epoch 262/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7824 - val_loss: 0.4152 - val_acc: 0.8098\n",
            "Epoch 263/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7821 - val_loss: 0.4151 - val_acc: 0.8205\n",
            "Epoch 264/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7825 - val_loss: 0.4151 - val_acc: 0.7741\n",
            "Epoch 265/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7851 - val_loss: 0.4154 - val_acc: 0.7970\n",
            "Epoch 266/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7872 - val_loss: 0.4154 - val_acc: 0.7686\n",
            "Epoch 267/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7877 - val_loss: 0.4155 - val_acc: 0.8113\n",
            "Epoch 268/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7904 - val_loss: 0.4153 - val_acc: 0.8054\n",
            "Epoch 269/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7881 - val_loss: 0.4171 - val_acc: 0.7549\n",
            "Epoch 270/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7860 - val_loss: 0.4152 - val_acc: 0.8059\n",
            "Epoch 271/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7843 - val_loss: 0.4152 - val_acc: 0.8050\n",
            "Epoch 272/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7894 - val_loss: 0.4153 - val_acc: 0.8206\n",
            "Epoch 273/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7869 - val_loss: 0.4150 - val_acc: 0.8220\n",
            "Epoch 274/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4153 - acc: 0.7866 - val_loss: 0.4151 - val_acc: 0.8165\n",
            "Epoch 275/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7902 - val_loss: 0.4152 - val_acc: 0.8204\n",
            "Epoch 276/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7924 - val_loss: 0.4153 - val_acc: 0.8160\n",
            "Epoch 277/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7871 - val_loss: 0.4150 - val_acc: 0.8078\n",
            "Epoch 278/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7917 - val_loss: 0.4152 - val_acc: 0.8242\n",
            "Epoch 279/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7895 - val_loss: 0.4153 - val_acc: 0.7714\n",
            "Epoch 280/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7860 - val_loss: 0.4150 - val_acc: 0.7755\n",
            "Epoch 281/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7852 - val_loss: 0.4150 - val_acc: 0.7706\n",
            "Epoch 282/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7869 - val_loss: 0.4151 - val_acc: 0.7744\n",
            "Epoch 283/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7864 - val_loss: 0.4150 - val_acc: 0.7735\n",
            "Epoch 284/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7855 - val_loss: 0.4155 - val_acc: 0.7634\n",
            "Epoch 285/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7858 - val_loss: 0.4151 - val_acc: 0.8237\n",
            "Epoch 286/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7847 - val_loss: 0.4151 - val_acc: 0.8119\n",
            "Epoch 287/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7856 - val_loss: 0.4153 - val_acc: 0.7704\n",
            "Epoch 288/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7852 - val_loss: 0.4152 - val_acc: 0.8235\n",
            "Epoch 289/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7832 - val_loss: 0.4149 - val_acc: 0.8203\n",
            "Epoch 290/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7852 - val_loss: 0.4151 - val_acc: 0.7890\n",
            "Epoch 291/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7847 - val_loss: 0.4150 - val_acc: 0.8054\n",
            "Epoch 292/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7841 - val_loss: 0.4151 - val_acc: 0.7848\n",
            "Epoch 293/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7858 - val_loss: 0.4156 - val_acc: 0.7657\n",
            "Epoch 294/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7882 - val_loss: 0.4150 - val_acc: 0.7742\n",
            "Epoch 295/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7872 - val_loss: 0.4150 - val_acc: 0.7735\n",
            "Epoch 296/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7866 - val_loss: 0.4150 - val_acc: 0.8038\n",
            "Epoch 297/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7851 - val_loss: 0.4152 - val_acc: 0.8032\n",
            "Epoch 298/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7835 - val_loss: 0.4150 - val_acc: 0.7920\n",
            "Epoch 299/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7850 - val_loss: 0.4151 - val_acc: 0.7973\n",
            "Epoch 300/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7865 - val_loss: 0.4156 - val_acc: 0.8125\n",
            "Epoch 301/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7885 - val_loss: 0.4152 - val_acc: 0.8277\n",
            "Epoch 302/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7862 - val_loss: 0.4155 - val_acc: 0.7722\n",
            "Epoch 303/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7843 - val_loss: 0.4165 - val_acc: 0.8052\n",
            "Epoch 304/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7884 - val_loss: 0.4149 - val_acc: 0.8288\n",
            "Epoch 305/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7875 - val_loss: 0.4151 - val_acc: 0.8210\n",
            "Epoch 306/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7857 - val_loss: 0.4149 - val_acc: 0.7735\n",
            "Epoch 307/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7877 - val_loss: 0.4154 - val_acc: 0.7709\n",
            "Epoch 308/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7866 - val_loss: 0.4151 - val_acc: 0.7682\n",
            "Epoch 309/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4152 - acc: 0.7870 - val_loss: 0.4150 - val_acc: 0.7766\n",
            "Epoch 310/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7883 - val_loss: 0.4151 - val_acc: 0.7852\n",
            "Epoch 311/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7856 - val_loss: 0.4151 - val_acc: 0.7963\n",
            "Epoch 312/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7864 - val_loss: 0.4154 - val_acc: 0.7714\n",
            "Epoch 313/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7845 - val_loss: 0.4200 - val_acc: 0.7732\n",
            "Epoch 314/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7857 - val_loss: 0.4148 - val_acc: 0.8284\n",
            "Epoch 315/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7863 - val_loss: 0.4150 - val_acc: 0.7713\n",
            "Epoch 316/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7855 - val_loss: 0.4152 - val_acc: 0.8225\n",
            "Epoch 317/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7854 - val_loss: 0.4149 - val_acc: 0.8218\n",
            "Epoch 318/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7851 - val_loss: 0.4152 - val_acc: 0.7844\n",
            "Epoch 319/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7863 - val_loss: 0.4152 - val_acc: 0.8116\n",
            "Epoch 320/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7892 - val_loss: 0.4152 - val_acc: 0.8200\n",
            "Epoch 321/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7877 - val_loss: 0.4172 - val_acc: 0.7524\n",
            "Epoch 322/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7864 - val_loss: 0.4150 - val_acc: 0.7691\n",
            "Epoch 323/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7871 - val_loss: 0.4150 - val_acc: 0.8223\n",
            "Epoch 324/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7854 - val_loss: 0.4149 - val_acc: 0.7881\n",
            "Epoch 325/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7848 - val_loss: 0.4149 - val_acc: 0.7800\n",
            "Epoch 326/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7871 - val_loss: 0.4151 - val_acc: 0.7877\n",
            "Epoch 327/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7850 - val_loss: 0.4152 - val_acc: 0.7825\n",
            "Epoch 328/500\n",
            "602/602 [==============================] - 2s 3ms/step - loss: 0.4151 - acc: 0.7836 - val_loss: 0.4149 - val_acc: 0.8266\n",
            "Epoch 329/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7868 - val_loss: 0.4149 - val_acc: 0.8124\n",
            "Epoch 330/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7873 - val_loss: 0.4148 - val_acc: 0.8304\n",
            "Epoch 331/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7868 - val_loss: 0.4152 - val_acc: 0.7778\n",
            "Epoch 332/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7870 - val_loss: 0.4165 - val_acc: 0.7796\n",
            "Epoch 333/500\n",
            "602/602 [==============================] - 2s 2ms/step - loss: 0.4149 - acc: 0.7865 - val_loss: 0.4147 - val_acc: 0.7885\n",
            "Epoch 334/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7861 - val_loss: 0.4152 - val_acc: 0.7894\n",
            "Epoch 335/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7935 - val_loss: 0.4152 - val_acc: 0.7867\n",
            "Epoch 336/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7865 - val_loss: 0.4159 - val_acc: 0.7719\n",
            "Epoch 337/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7865 - val_loss: 0.4154 - val_acc: 0.8149\n",
            "Epoch 338/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7866 - val_loss: 0.4150 - val_acc: 0.7752\n",
            "Epoch 339/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7865 - val_loss: 0.4148 - val_acc: 0.8266\n",
            "Epoch 340/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7884 - val_loss: 0.4148 - val_acc: 0.7763\n",
            "Epoch 341/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7876 - val_loss: 0.4151 - val_acc: 0.8179\n",
            "Epoch 342/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7856 - val_loss: 0.4148 - val_acc: 0.7772\n",
            "Epoch 343/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7858 - val_loss: 0.4149 - val_acc: 0.8168\n",
            "Epoch 344/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7845 - val_loss: 0.4150 - val_acc: 0.7734\n",
            "Epoch 345/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4151 - acc: 0.7854 - val_loss: 0.4151 - val_acc: 0.8248\n",
            "Epoch 346/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7849 - val_loss: 0.4149 - val_acc: 0.7740\n",
            "Epoch 347/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7859 - val_loss: 0.4148 - val_acc: 0.7775\n",
            "Epoch 348/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7874 - val_loss: 0.4149 - val_acc: 0.8250\n",
            "Epoch 349/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7834 - val_loss: 0.4149 - val_acc: 0.7938\n",
            "Epoch 350/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7895 - val_loss: 0.4148 - val_acc: 0.8154\n",
            "Epoch 351/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7864 - val_loss: 0.4152 - val_acc: 0.8206\n",
            "Epoch 352/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7848 - val_loss: 0.4149 - val_acc: 0.8265\n",
            "Epoch 353/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7865 - val_loss: 0.4154 - val_acc: 0.8218\n",
            "Epoch 354/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7853 - val_loss: 0.4149 - val_acc: 0.7707\n",
            "Epoch 355/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7865 - val_loss: 0.4153 - val_acc: 0.7704\n",
            "Epoch 356/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7868 - val_loss: 0.4151 - val_acc: 0.8157\n",
            "Epoch 357/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7869 - val_loss: 0.4151 - val_acc: 0.8210\n",
            "Epoch 358/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7861 - val_loss: 0.4148 - val_acc: 0.7854\n",
            "Epoch 359/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7863 - val_loss: 0.4151 - val_acc: 0.8226\n",
            "Epoch 360/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7852 - val_loss: 0.4149 - val_acc: 0.8237\n",
            "Epoch 361/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4148 - acc: 0.7864 - val_loss: 0.4149 - val_acc: 0.8142\n",
            "Epoch 362/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7874 - val_loss: 0.4149 - val_acc: 0.8199\n",
            "Epoch 363/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4148 - acc: 0.7854 - val_loss: 0.4151 - val_acc: 0.7867\n",
            "Epoch 364/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7863 - val_loss: 0.4149 - val_acc: 0.7880\n",
            "Epoch 365/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4150 - acc: 0.7845 - val_loss: 0.4147 - val_acc: 0.8033\n",
            "Epoch 366/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7849 - val_loss: 0.4149 - val_acc: 0.8219\n",
            "Epoch 367/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7854 - val_loss: 0.4147 - val_acc: 0.8298\n",
            "Epoch 368/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7848 - val_loss: 0.4148 - val_acc: 0.7761\n",
            "Epoch 369/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7846 - val_loss: 0.4149 - val_acc: 0.8062\n",
            "Epoch 370/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4148 - acc: 0.7856 - val_loss: 0.4152 - val_acc: 0.8181\n",
            "Epoch 371/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7863 - val_loss: 0.4150 - val_acc: 0.8202\n",
            "Epoch 372/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7867 - val_loss: 0.4149 - val_acc: 0.8045\n",
            "Epoch 373/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7864 - val_loss: 0.4147 - val_acc: 0.7850\n",
            "Epoch 374/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7845 - val_loss: 0.4152 - val_acc: 0.7763\n",
            "Epoch 375/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7860 - val_loss: 0.4147 - val_acc: 0.8206\n",
            "Epoch 376/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4148 - acc: 0.7865 - val_loss: 0.4148 - val_acc: 0.7703\n",
            "Epoch 377/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7872 - val_loss: 0.4148 - val_acc: 0.7748\n",
            "Epoch 378/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7839 - val_loss: 0.4149 - val_acc: 0.8227\n",
            "Epoch 379/500\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7839 - val_loss: 0.4147 - val_acc: 0.7753\n",
            "Epoch 380/500\n",
            "592/602 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.7882Restoring model weights from the end of the best epoch: 330.\n",
            "602/602 [==============================] - 1s 2ms/step - loss: 0.4149 - acc: 0.7883 - val_loss: 0.4148 - val_acc: 0.8281\n",
            "Epoch 380: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = autoencoder.fit(\n",
        "    X_train, X_train,\n",
        "    shuffle=True,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=cb,\n",
        "    validation_data=(X_validate, X_validate)\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiJ_DnZKrglw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EPz4xOOrglx",
        "outputId": "48658083-7cc3-4989-b734-b28c799205d5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 훈련 과정 시각화 (정확도)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# 훈련 과정 시각화 (손실)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z43q3RTQrgly"
      },
      "outputs": [],
      "source": [
        "# pass the transformed test set through the autoencoder to get the reconstructed result\n",
        "reconstructions = autoencoder.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCznzbQKrgly"
      },
      "outputs": [],
      "source": [
        "# calculating the mean squared error reconstruction loss per row in the numpy array\n",
        "mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lUTKKKUrgly",
        "outputId": "304e137d-9e82-428f-d3ab-fc547eaed5d1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "normal = mse[y_test==0]\n",
        "abnormal = mse[y_test==1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "\n",
        "ax.hist(normal, bins=50, density=True, label=\"normal\", alpha=.6, color=\"green\")\n",
        "ax.hist(abnormal, bins=50, density=True, label=\"abnormal\", alpha=.6, color=\"red\")\n",
        "\n",
        "plt.title(\"(Normalized) Distribution of the Reconstruction Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQDLBDUkrglz"
      },
      "source": [
        "### 1) Threshold = max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfHrCw01rglz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (confusion_matrix, \n",
        "                             precision_recall_curve, \n",
        "                             classification_report,\n",
        "                             f1_score, precision_score, recall_score, accuracy_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnT5IZ-mrglz"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = max(normal)\n",
        "\n",
        "outliers = mse > THRESHOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nGjlSegrglz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (confusion_matrix, \n",
        "                             precision_recall_curve, f1_score)\n",
        "\n",
        "# get (mis)classification\n",
        "cm = confusion_matrix(y_test, outliers)\n",
        "\n",
        "# true/false positives/negatives\n",
        "(tn, fp, \n",
        " fn, tp) = cm.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQSSNipvrgl0",
        "outputId": "374d9331-37b0-490f-be56-8ba6b933839e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      1.00      0.68       473\n",
            "           1       1.00      0.04      0.07       473\n",
            "\n",
            "    accuracy                           0.52       946\n",
            "   macro avg       0.75      0.52      0.37       946\n",
            "weighted avg       0.75      0.52      0.37       946\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Classification Report :\")\n",
        "print(classification_report(y_test, outliers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi4sR3HNrgl0",
        "outputId": "c405033d-78a8-422f-f47a-05d4492a1afb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The classifications using the MAD method with threshold=2.527491663747746 are as follows:\n",
            "[[473   0]\n",
            " [455  18]]\n",
            "\n",
            "% of transactions labeled as fraud that were correct (precision): 18/(0+18) = 100.00%\n",
            "% of fraudulent transactions were caught succesfully (recall):    18/(455+18) = 3.81%\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"The classifications using the MAD method with threshold={THRESHOLD} are as follows:\n",
        "{cm}\n",
        "\n",
        "% of transactions labeled as fraud that were correct (precision): {tp}/({fp}+{tp}) = {tp/(fp+tp):.2%}\n",
        "% of fraudulent transactions were caught succesfully (recall):    {tp}/({fn}+{tp}) = {tp/(fn+tp):.2%}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csIz4YGxrgl0",
        "outputId": "21cb294e-fad7-49e2-a343-a8617042d680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1 score: 0.07331975560081466\n",
            "precision: 1.0\n",
            "recall: 0.03805496828752643\n",
            "accuracy: 0.5190274841437632\n"
          ]
        }
      ],
      "source": [
        "print(\"f1 score:\", f1_score(y_test, outliers))\n",
        "print(\"precision:\", precision_score(y_test, outliers))\n",
        "print(\"recall:\", recall_score(y_test, outliers))\n",
        "print(\"accuracy:\", accuracy_score(y_test, outliers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb0-1Nsjrgl0"
      },
      "source": [
        "### 2) Threshold = Q3 + 1.5*IQR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1Jr8ZONrgl1",
        "outputId": "c8adae63-d3a6-4c51-9fae-82bd9d2964f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2533604326679433\n"
          ]
        }
      ],
      "source": [
        "quantile_25 = np.percentile(normal, 25)\n",
        "quantile_75 = np.percentile(normal, 75)\n",
        "\n",
        "IQR_weight = 1.5*(quantile_75 - quantile_25)\n",
        "\n",
        "THRESHOLD = quantile_75 + IQR_weight\n",
        "print(THRESHOLD)\n",
        "\n",
        "outliers = mse > THRESHOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StyR1Bw1rgl1"
      },
      "outputs": [],
      "source": [
        "# get (mis)classification\n",
        "cm = confusion_matrix(y_test, outliers)\n",
        "\n",
        "# true/false positives/negatives\n",
        "(tn, fp, \n",
        " fn, tp) = cm.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "936gLgfsrgl1",
        "outputId": "678c34f9-9d0f-49c5-ea80-10ce3d2b7856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       473\n",
            "           1       0.94      0.90      0.92       473\n",
            "\n",
            "    accuracy                           0.92       946\n",
            "   macro avg       0.92      0.92      0.92       946\n",
            "weighted avg       0.92      0.92      0.92       946\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Classification Report :\")\n",
        "print(classification_report(y_test, outliers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yASm5PHprgl1",
        "outputId": "b17afcd0-1343-4780-a9cb-72387a9e52cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The classifications using the MAD method with threshold=1.2533604326679433 are as follows:\n",
            "[[448  25]\n",
            " [ 47 426]]\n",
            "\n",
            "% of transactions labeled as fraud that were correct (precision): 426/(25+426) = 94.46%\n",
            "% of fraudulent transactions were caught succesfully (recall):    426/(47+426) = 90.06%\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"The classifications using the MAD method with threshold={THRESHOLD} are as follows:\n",
        "{cm}\n",
        "\n",
        "% of transactions labeled as fraud that were correct (precision): {tp}/({fp}+{tp}) = {tp/(fp+tp):.2%}\n",
        "% of fraudulent transactions were caught succesfully (recall):    {tp}/({fn}+{tp}) = {tp/(fn+tp):.2%}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obWeMuBkrgl2",
        "outputId": "239b6fee-61a1-40d8-df0c-6abe95f5e971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1 score: 0.9220779220779222\n",
            "precision: 0.9445676274944568\n",
            "recall: 0.9006342494714588\n",
            "accuracy: 0.9238900634249472\n"
          ]
        }
      ],
      "source": [
        "print(\"f1 score:\", f1_score(y_test, outliers))\n",
        "print(\"precision:\", precision_score(y_test, outliers))\n",
        "print(\"recall:\", recall_score(y_test, outliers))\n",
        "print(\"accuracy:\", accuracy_score(y_test, outliers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1eOAj3jrgl2"
      },
      "source": [
        "### 3) Heuristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhdv8vvFrgl2"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = 1\n",
        "\n",
        "outliers = mse > THRESHOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUijpoG8rgl2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (confusion_matrix, \n",
        "                             precision_recall_curve)\n",
        "\n",
        "# get (mis)classification\n",
        "cm = confusion_matrix(y_test, outliers)\n",
        "\n",
        "# true/false positives/negatives\n",
        "(tn, fp, \n",
        " fn, tp) = cm.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyRO3gHOrgl3",
        "outputId": "19c1cdcf-e5b7-45b2-de81-750729b6c94f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94       473\n",
            "           1       0.91      0.98      0.94       473\n",
            "\n",
            "    accuracy                           0.94       946\n",
            "   macro avg       0.94      0.94      0.94       946\n",
            "weighted avg       0.94      0.94      0.94       946\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Classification Report :\")\n",
        "print(classification_report(y_test, outliers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gQuk-VGrgl3",
        "outputId": "4149b396-4d34-4c13-d819-e4b80a8fb583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The classifications using the MAD method with threshold=1 are as follows:\n",
            "[[426  47]\n",
            " [  8 465]]\n",
            "\n",
            "% of transactions labeled as fraud that were correct (precision): 465/(47+465) = 90.82%\n",
            "% of fraudulent transactions were caught succesfully (recall):    465/(8+465) = 98.31%\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"The classifications using the MAD method with threshold={THRESHOLD} are as follows:\n",
        "{cm}\n",
        "\n",
        "% of transactions labeled as fraud that were correct (precision): {tp}/({fp}+{tp}) = {tp/(fp+tp):.2%}\n",
        "% of fraudulent transactions were caught succesfully (recall):    {tp}/({fn}+{tp}) = {tp/(fn+tp):.2%}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQijwWo5rgl3",
        "outputId": "ba59c67f-5907-412b-d3a5-fd644e13dd0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1 score: 0.9441624365482234\n",
            "precision: 0.908203125\n",
            "recall: 0.9830866807610994\n",
            "accuracy: 0.9418604651162791\n"
          ]
        }
      ],
      "source": [
        "print(\"f1 score:\", f1_score(y_test, outliers))\n",
        "print(\"precision:\", precision_score(y_test, outliers))\n",
        "print(\"recall:\", recall_score(y_test, outliers))\n",
        "print(\"accuracy:\", accuracy_score(y_test, outliers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyO5Q9Snrgl4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Autoencoder 실험 2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}